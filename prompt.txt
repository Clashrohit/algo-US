##backtest.py##


import numpy as np
import pandas as pd


def backtest_equity_single(
    df: pd.DataFrame,
    symbol: str,
    price_col: str = "Close",
    signal_col: str = "signal",
    date_col: str = "date",
    initial: float = 10000.0,
    allow_short: bool = False
):
    """
    Single-stock backtest.
    Use when df has multiple symbols. We filter one symbol and backtest it.
    """

    d = df.copy()
    if "symbol" in d.columns:
        d = d[d["symbol"] == symbol].copy()
    if d.empty:
        raise ValueError(f"No rows found for symbol={symbol}")

    # sort by date
    if date_col in d.columns:
        d[date_col] = pd.to_datetime(d[date_col], errors="coerce")
        d = d.dropna(subset=[date_col]).sort_values(date_col).reset_index(drop=True)
    else:
        d = d.sort_index()

    # returns
    d["ret"] = d[price_col].pct_change().fillna(0.0)

    # position from signal (next day execution)
    raw_pos = d[signal_col].shift(1).fillna(0.0)

    if allow_short:
        d["pos"] = raw_pos.clip(-1, 1).astype(float)
    else:
        d["pos"] = (raw_pos > 0).astype(float)   # -1 => exit

    d["strat_ret"] = d["pos"] * d["ret"]
    d["equity"] = (1 + d["strat_ret"]).cumprod() * initial

    total_return = float(d["equity"].iloc[-1] / initial - 1)
    roll_max = d["equity"].cummax()
    dd = d["equity"] / roll_max - 1
    max_dd = float(dd.min())

    r = d["strat_ret"].fillna(0.0)
    sharpe = 0.0 if r.std() == 0 else float(np.sqrt(252) * r.mean() / r.std())
    win_rate = float((r[r != 0] > 0).mean()) if (r != 0).any() else 0.0

    metrics = {
        "symbol": symbol,
        "total_return": total_return,
        "max_drawdown": max_dd,
        "sharpe": sharpe,
        "win_rate": win_rate
    }

    return d, metrics


def backtest_topn_portfolio(
    df: pd.DataFrame,
    top_n: int = 10,
    price_col: str = "Close",
    prob_col: str = "prob_buy",
    date_col: str = "date",
    symbol_col: str = "symbol",
    initial: float = 10000.0
):
    """
    Multi-stock portfolio backtest using Top-N selection by prob_buy each day.

    Execution:
      - Select top_n stocks by prob_buy at day t-1 (signal generation)
      - Hold them on day t (next day execution) to avoid lookahead

    Required columns: symbol, date, Close, prob_buy
    """

    d = df.copy()
    d[date_col] = pd.to_datetime(d[date_col], errors="coerce")
    d = d.dropna(subset=[date_col, symbol_col, price_col, prob_col]).copy()

    # sort by symbol/date
    d = d.sort_values([symbol_col, date_col]).reset_index(drop=True)

    # per-symbol daily return
    d["ret"] = d.groupby(symbol_col)[price_col].pct_change().fillna(0.0)

    # rank within each date by prob_buy (higher is better)
    d["rank"] = d.groupby(date_col)[prob_col].rank(ascending=False, method="first")

    # selected_today is based on today's prob_buy (decision time)
    d["selected_today"] = (d["rank"] <= top_n).astype(int)

    # execute next day: pos_t = selected_{t-1}
    d["pos"] = d.groupby(symbol_col)["selected_today"].shift(1).fillna(0).astype(int)

    # portfolio return = mean return across held positions each day (equal weight)
    def _port_ret(g):
        held = g[g["pos"] == 1]
        if len(held) == 0:
            return 0.0
        return float(held["ret"].mean())

    all_dates = pd.Series(sorted(d[date_col].unique()), name=date_col)

    held = d[d["pos"] == 1].copy()
    daily_ret = held.groupby(date_col)["ret"].mean()

    daily = all_dates.to_frame()
    daily["port_ret"] = daily[date_col].map(daily_ret).fillna(0.0)
    daily = daily.sort_values(date_col).reset_index(drop=True)

    daily["equity"] = (1 + daily["port_ret"]).cumprod() * initial

    total_return = float(daily["equity"].iloc[-1] / initial - 1)
    roll_max = daily["equity"].cummax()
    dd = daily["equity"] / roll_max - 1
    max_dd = float(dd.min())

    r = daily["port_ret"].fillna(0.0)
    sharpe = 0.0 if r.std() == 0 else float(np.sqrt(252) * r.mean() / r.std())
    win_rate = float((r[r != 0] > 0).mean()) if (r != 0).any() else 0.0

    metrics = {
        "top_n": top_n,
        "total_return": total_return,
        "max_drawdown": max_dd,
        "sharpe": sharpe,
        "win_rate": win_rate
    }

    return daily, metrics



##bulk_download_prices.py##

import os
import time
import ssl
import pandas as pd
import yfinance as yf

# SSL workaround (for proxy/self-signed environments)
ssl._create_default_https_context = ssl._create_unverified_context

START = "2010-01-01"
END   = "2026-01-07"  # end exclusive in yfinance
OUT_DIR = os.path.join("data", "prices")
os.makedirs(OUT_DIR, exist_ok=True)

# Load tickers from your SP500.csv
sp500_df = pd.read_csv("SP500.csv")
tickers = sp500_df["Symbol"].astype(str).str.strip().str.upper().tolist()
tickers = [t.replace(".", "-") for t in tickers if t and t != "NAN"]  # BRK.B -> BRK-B

def yf_download_retry(symbol: str, retries: int = 5, sleep_sec: float = 2.0):
    # try increase yfinance timeout (works in many versions)
    try:
        import yfinance.shared as shared
        shared._DEFAULTS["timeout"] = 60
    except Exception:
        pass

    last_err = None
    for i in range(1, retries + 1):
        try:
            df = yf.download(
                symbol,
                start=START,
                end=END,
                auto_adjust=True,
                progress=False,
                threads=False,     # proxy-friendly
            )
            if df is not None and not df.empty:
                df = df[["Open","High","Low","Close","Volume"]].dropna()
                return df
            last_err = "empty"
        except Exception as e:
            last_err = str(e)

        print(f"[WARN] {symbol} attempt {i}/{retries} failed: {last_err}")
        time.sleep(sleep_sec)

    return None

failed = []
ok = 0

for idx, sym in enumerate(tickers, start=1):
    out_path = os.path.join(OUT_DIR, f"{sym}.csv")

    # skip if already downloaded
    if os.path.exists(out_path) and os.path.getsize(out_path) > 1000:
        print(f"[SKIP] {sym} exists")
        continue

    print(f"[{idx}/{len(tickers)}] Downloading {sym} ...")
    df = yf_download_retry(sym)

    if df is None or df.empty:
        failed.append(sym)
        continue

    df.to_csv(out_path, index=True)
    ok += 1

    # rate limit (important)
    time.sleep(0.8)

print("\nDONE")
print("Downloaded:", ok)
print("Failed:", len(failed))
if failed:
    with open("failed_tickers.txt", "w") as f:
        for s in failed:
            f.write(s + "\n")
    print("Saved failed tickers to failed_tickers.txt")



##config.py##

import os
import pandas as pd
from typing import List

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "outputs")
DATA_DIR = os.path.join(BASE_DIR, "data")
PRICES_DIR = os.path.join(DATA_DIR, "prices")
SP500_CSV = os.path.join(BASE_DIR, "SP500.csv")

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(PRICES_DIR, exist_ok=True)

START_DATE = "2010-01-01"
END_DATE = "2023-12-31"

MAX_TICKERS = 50
HORIZON_DAYS = 5
N_SPLITS = 5

CORR_THRESHOLD = 0.75

# Signal thresholds
BUY_THR = 0.52
SELL_THR = 0.40

TABLE3_TECH_COLS = [
    "RSI", "STOCH_K", "CCI", "CMO", "COPP", "PPO", "MACD", "EMA_20", "KAMA_10",
    "SMA_20", "SMA_50", "VAMA_20", "TRIMA_20",
    "ICHIMOKU_conv", "ICHIMOKU_base", "ICHIMOKU_span_a", "ICHIMOKU_span_b",
    "BB_upper", "OBV", "MFI_14",
]

TABLE2_MIN_COLS = ["US_Score", "Total_Trend"]

def load_sp500_tickers() -> List[str]:
    sp500_df = pd.read_csv(SP500_CSV)
    if "Symbol" not in sp500_df.columns:
        raise ValueError("SP500.csv must contain 'Symbol' column.")
    tickers = sp500_df["Symbol"].astype(str).tolist()
    tickers = [t.strip().upper().replace(".", "-") for t in tickers if t and t.strip() and t != "nan"]
    # dedupe
    out, seen = [], set()
    for t in tickers:
        if t not in seen:
            seen.add(t)
            out.append(t)
    return out[:MAX_TICKERS]


##data_loader.py##

import os
import time
import ssl
from typing import Optional

import pandas as pd
import yfinance as yf

from config import PRICES_DIR

ssl._create_default_https_context = ssl._create_unverified_context


def _finalize_ohlcv(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    if df is None or df.empty:
        return None

    df = df.copy()
    df.index = pd.to_datetime(df.index, errors="coerce")
    df = df[df.index.notna()]
    df = df.sort_index()
    df = df[~df.index.duplicated(keep="first")]

    needed = ["Open", "High", "Low", "Close", "Volume"]
    if not all(c in df.columns for c in needed):
        return None

    df = df[needed].copy()
    for c in needed:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    df = df.replace([pd.NA, float("inf"), float("-inf")], pd.NA).dropna()
    return df


def _parse_standard_csv(path: str) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(path)
        if "Date" not in df.columns:
            return None
        df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
        df = df.dropna(subset=["Date"]).set_index("Date")
        return _finalize_ohlcv(df)
    except Exception:
        return None


def _parse_yfinance_multiheader_csv(path: str) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(path, header=[0, 1], index_col=0)
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        return _finalize_ohlcv(df)
    except Exception:
        return None


def load_price_csv(symbol: str) -> Optional[pd.DataFrame]:
    path = os.path.join(PRICES_DIR, f"{symbol}.csv")
    if not os.path.exists(path):
        return None

    df = _parse_standard_csv(path)
    if df is not None and not df.empty:
        return df

    df = _parse_yfinance_multiheader_csv(path)
    if df is not None and not df.empty:
        return df

    print(f"[WARN] Could not parse CSV format for {symbol}: {path}")
    return None


def yf_download_retry(symbol: str, start: str, end: str, retries: int = 3) -> Optional[pd.DataFrame]:
    last_err = None
    for i in range(1, retries + 1):
        try:
            df = yf.download(symbol, start=start, end=end, auto_adjust=True, progress=False, threads=False)
            if df is not None and not df.empty:
                return _finalize_ohlcv(df)
            last_err = "empty"
        except Exception as e:
            last_err = str(e)
        print(f"[WARN] yfinance failed {symbol} ({i}/{retries}): {last_err}")
        time.sleep(1.5)
    return None


def get_price_data(symbol: str, start: str, end: str, offline_only: bool = True) -> Optional[pd.DataFrame]:
    df = load_price_csv(symbol)
    if df is not None and not df.empty:
        start_dt = pd.to_datetime(start)
        end_dt = pd.to_datetime(end)
        df = df[(df.index >= start_dt) & (df.index < end_dt)]
        if not df.empty:
            return df

    if offline_only:
        return None

    return yf_download_retry(symbol, start, end)



##dataset_builder.py##
import numpy as np
import pandas as pd
from typing import List

from config import START_DATE, END_DATE, OUTPUT_DIR, TABLE3_TECH_COLS, TABLE2_MIN_COLS
from data_loader import get_price_data
from indicators import add_indicators_table3, compute_us_score


def _add_regime_feature(d: pd.DataFrame, lookback: int = 20, eps: float = 0.0005) -> pd.DataFrame:
    """
    Regime (past-only):
      2 = Bull, 1 = Sideways, 0 = Bear
    Based on rolling mean of log returns.
    """
    out = d.copy()
    r = np.log(out["Close"] / out["Close"].shift(1))
    mu = r.rolling(lookback).mean()

    regime = np.ones(len(out), dtype=int)
    regime[mu > eps] = 2
    regime[mu < -eps] = 0
    out["Regime"] = regime.astype(int)
    return out


def build_timeseries_dataset(
    tickers: List[str],
    horizon_days: int = 5,
    offline_only: bool = True,
    save_file: bool = False,
    add_regime: bool = True,
    regime_lookback: int = 20,
    regime_eps: float = 0.0005
) -> pd.DataFrame:
    frames = []

    for sym in tickers:
        df = get_price_data(sym, START_DATE, END_DATE, offline_only=offline_only)
        if df is None or df.empty or df.shape[0] < 400:
            continue

        d = add_indicators_table3(df).sort_index()
        if d.empty:
            continue

        if add_regime:
            d = _add_regime_feature(d, lookback=regime_lookback, eps=regime_eps)

        future_close = d["Close"].shift(-horizon_days)
        valid = future_close.notna()
        if valid.sum() == 0:
            continue

        d = d.loc[valid].copy()
        d["label"] = (future_close.loc[valid] > d["Close"]).astype(int)

        if "US_Score" in TABLE2_MIN_COLS:
            d["US_Score"] = d.apply(compute_us_score, axis=1).astype(float)

        if "Total_Trend" in TABLE2_MIN_COLS:
            first_close = d["Close"].iloc[0]
            d["Total_Trend"] = (d["Close"] / first_close - 1.0).astype(float)

        use_cols = (
            ["Close"]
            + TABLE3_TECH_COLS
            + (["Regime"] if add_regime else [])
            + [c for c in TABLE2_MIN_COLS if c in d.columns]
            + ["label"]
        )
        use_cols = [c for c in use_cols if c in d.columns]

        out = d[use_cols].copy()
        out["symbol"] = sym
        out["date"] = out.index
        frames.append(out.reset_index(drop=True))

    out_all = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()

    if save_file and not out_all.empty:
        out_all.to_csv(f"{OUTPUT_DIR}/timeseries_dataset.csv", index=False)

    return out_all


def build_us_timeseries_dataset(
    tickers: List[str],
    offline_only: bool = True,
    us_threshold: int = 3,
    save_file: bool = False,
    add_regime: bool = True,
    regime_lookback: int = 20,
    regime_eps: float = 0.0005
) -> pd.DataFrame:
    frames = []

    for sym in tickers:
        df = get_price_data(sym, START_DATE, END_DATE, offline_only=offline_only)
        if df is None or df.empty or df.shape[0] < 400:
            continue

        d = add_indicators_table3(df).sort_index()
        if d.empty:
            continue

        if add_regime:
            d = _add_regime_feature(d, lookback=regime_lookback, eps=regime_eps)

        score = (
            (d["Close"] < d["SMA_20"]).astype(int)
            + (d["Close"] < d["SMA_50"]).astype(int)
            + (d["RSI"] < 40).astype(int)
        )
        disc = 1 - (d["Close"] / d["52w_high"])
        score = score + (disc > 0.20).astype(int)

        d["US_Score"] = score.astype(float)
        d["label"] = (score >= us_threshold).astype(int)

        if "Total_Trend" in TABLE2_MIN_COLS:
            first_close = d["Close"].iloc[0]
            d["Total_Trend"] = (d["Close"] / first_close - 1.0).astype(float)

        use_cols = (
            ["Close"]
            + TABLE3_TECH_COLS
            + (["Regime"] if add_regime else [])
            + [c for c in TABLE2_MIN_COLS if c in d.columns]
            + ["US_Score", "label"]
        )
        use_cols = [c for c in use_cols if c in d.columns]

        out = d[use_cols].copy()
        out["symbol"] = sym
        out["date"] = out.index
        frames.append(out.reset_index(drop=True))

    out_all = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()

    if save_file and not out_all.empty:
        out_all.to_csv(f"{OUTPUT_DIR}/us_timeseries_dataset.csv", index=False)

    return out_all


##feature_selection.py##

import numpy as np
import pandas as pd
from typing import Tuple, List

from config import TABLE3_TECH_COLS, TABLE2_MIN_COLS, CORR_THRESHOLD


def paper_feature_select(df: pd.DataFrame, keep_id_cols: bool = True) -> Tuple[pd.DataFrame, List[str], List[str]]:
    d = df.copy()

    if "MA20_MA50" not in d.columns and ("SMA_20" in d.columns and "SMA_50" in d.columns):
        d["MA20_MA50"] = (d["SMA_20"] - d["SMA_50"]).astype(float)

    paper_cols = ["MA20_MA50", "RSI", "MACD", "BB_upper", "OBV", "ICHIMOKU_base"]
    keep = [c for c in paper_cols if c in d.columns]
    missing = [c for c in paper_cols if c not in d.columns]

    if "Regime" in d.columns:
        keep.append("Regime")

    cols = []
    if keep_id_cols:
        for c in ["symbol", "date"]:
            if c in d.columns:
                cols.append(c)

    if "Close" in d.columns:
        cols.append("Close")

    cols += keep

    for c in TABLE2_MIN_COLS:
        if c in d.columns:
            cols.append(c)

    cols.append("label")
    return d[cols].copy(), keep, missing


def correlation_select(df: pd.DataFrame, threshold: float = CORR_THRESHOLD, keep_id_cols: bool = True) -> Tuple[pd.DataFrame, List[str], List[str]]:
    tech_df = df[TABLE3_TECH_COLS].copy()
    corr = tech_df.corr(numeric_only=True).abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))

    drop = [c for c in upper.columns if any(upper[c] > threshold)]
    keep = [c for c in TABLE3_TECH_COLS if c not in drop]

    cols = []
    if keep_id_cols:
        for c in ["symbol", "date"]:
            if c in df.columns:
                cols.append(c)

    if "Close" in df.columns:
        cols.append("Close")

    cols += keep
    if "Regime" in df.columns:
        cols.append("Regime")

    for c in TABLE2_MIN_COLS:
        if c in df.columns:
            cols.append(c)

    cols.append("label")
    return df[cols].copy(), keep, drop


##figures_1_to_3.py##

import os
import time
import ssl
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf

plt.style.use("seaborn-v0_8")

# SSL workaround (proxy/self-signed)
ssl._create_default_https_context = ssl._create_unverified_context

DATA_DIR = "data"
PRICES_DIR = os.path.join(DATA_DIR, "prices")
OUTPUT_DIR = "outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Period shown in your paper figures (change if needed)
START = "2022-01-01"
END   = "2023-01-31"


# ---------- CSV loader (supports both normal + your multi-header format) ----------

def load_price_csv_any(symbol: str):
    path = os.path.join(PRICES_DIR, f"{symbol}.csv")
    if not os.path.exists(path):
        return None

    # Try standard
    try:
        df0 = pd.read_csv(path)
        if "Date" in df0.columns:
            df0["Date"] = pd.to_datetime(df0["Date"], errors="coerce")
            df0 = df0.dropna(subset=["Date"]).set_index("Date")
            need = ["Open", "High", "Low", "Close", "Volume"]
            if all(c in df0.columns for c in need):
                df0 = df0[need].apply(pd.to_numeric, errors="coerce").dropna()
                return df0
    except Exception:
        pass

    # Try yfinance multi-header
    try:
        df = pd.read_csv(path, header=[0, 1], index_col=0)
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        df.index = pd.to_datetime(df.index, errors="coerce")
        df = df[df.index.notna()]
        need = ["Open", "High", "Low", "Close", "Volume"]
        for c in need:
            df[c] = pd.to_numeric(df[c], errors="coerce")
        df = df[need].dropna()
        return df
    except Exception:
        return None


# ---------- yfinance fallback with retry (won't crash) ----------

def yf_download_retry(symbol: str, start: str, end: str, retries: int = 3):
    try:
        import yfinance.shared as shared
        shared._DEFAULTS["timeout"] = 60
    except Exception:
        pass

    last = None
    for i in range(1, retries + 1):
        try:
            df = yf.download(symbol, start=start, end=end, auto_adjust=True,
                             progress=False, threads=False)
            if df is not None and not df.empty:
                df = df[["Open", "High", "Low", "Close", "Volume"]].dropna()
                return df
            last = "empty"
        except Exception as e:
            last = str(e)
        print(f"[WARN] Download failed {symbol} ({i}/{retries}): {last}")
        time.sleep(1.5)
    return None


def get_close_series(symbol: str, start: str, end: str):
    # offline first
    df = load_price_csv_any(symbol)
    if df is not None and not df.empty:
        df = df[(df.index >= pd.to_datetime(start)) & (df.index < pd.to_datetime(end))]
        if not df.empty:
            return df["Close"]

    # online fallback
    df = yf_download_retry(symbol, start, end, retries=3)
    if df is None or df.empty:
        return None
    return df["Close"]


# ---------- Plot like paper Figure 1/2/3 ----------

def plot_index_figure(tickers, title, out_path, start=START, end=END):
    series_list = []
    names = []

    for t in tickers:
        s = get_close_series(t, start, end)
        if s is None or s.empty:
            print(f"[SKIP] No data for {t}")
            continue
        series_list.append(s)
        names.append(t)

    if len(series_list) == 0:
        print(f"[ERROR] No ticker data available for: {title}")
        return

    # Align by date
    df_close = pd.concat(series_list, axis=1)
    df_close.columns = names
    df_close.dropna(inplace=True)

    # Normalized return (start=0)
    norm = (df_close / df_close.iloc[0]) - 1.0

    dates = norm.index
    x = np.arange(len(dates))

    plt.figure(figsize=(10, 3.5))
    for col in norm.columns:
        y = norm[col].values
        plt.plot(dates, y, label=col)

        # dotted linear trend
        m, b = np.polyfit(x, y, 1)
        plt.plot(dates, m * x + b, linestyle=":", color="k", linewidth=1)

    plt.title(title)
    plt.xlabel("Date")
    plt.ylabel("Cumulative return")
    plt.grid(True, alpha=0.3)
    plt.legend(loc="best")
    plt.tight_layout()
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close()
    print("Saved:", out_path)


def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # Paper selections (change if you want)
    sp500 = ["HES", "ON", "SLB", "XOM"]
    nasdaq100 = ["CDNS", "MNST", "ODFL", "ROST", "VRTX"]
    dow = ["CVX", "MCD", "TRV"]

    plot_index_figure(
        sp500,
        title="FIGURE 1. S&P 500.",
        out_path=os.path.join(OUTPUT_DIR, "FIGURE_1_SP500.png")
    )

    plot_index_figure(
        nasdaq100,
        title="FIGURE 2. NASDAQ100.",
        out_path=os.path.join(OUTPUT_DIR, "FIGURE_2_NASDAQ100.png")
    )

    plot_index_figure(
        dow,
        title="FIGURE 3. Dow Jones.",
        out_path=os.path.join(OUTPUT_DIR, "FIGURE_3_DOWJONES.png")
    )


if __name__ == "__main__":
    main()


##figures_5_to_11.py##

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.ensemble import GradientBoostingClassifier
try:
    from sklearn.ensemble import HistGradientBoostingClassifier
    HGB_AVAILABLE = True
except Exception:
    HGB_AVAILABLE = False

plt.style.use("seaborn-v0_8")

# ================= CONFIG =================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PRICES_DIR = os.path.join(BASE_DIR, "data", "prices")
OUTPUT_DIR = os.path.join(BASE_DIR, "outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

SYMBOL = "AAPL"

START_DISPLAY = "2014-01-02"
END_DISPLAY   = "2024-10-14"       # inclusive display
END_EXCL      = "2024-10-15"       # exclusive filter end

INITIAL_CAPITAL = 10_000.0

# -------- ML (fast + paper-like risk filter) --------
LABEL_HORIZON_DAYS = 5
LABEL_RET_THR = 0.0         # 5-day direction label: >0 means up
WARMUP_BARS = 252
RETRAIN_EVERY = 21          # monthly retrain (fast + better)
ENTER_THR = 0.53            # re-enter long if prob >= ENTER_THR
EXIT_THR  = 0.045          # exit to cash if prob <= EXIT_THR
START_IN_MARKET = 1         # start long like baseline
# ----------------------------------------------------


# ================= TABLE/FIGURE HELPERS =================
def save_table_png(df: pd.DataFrame, title: str, out_path: str, font_size: int = 10):
    fig, ax = plt.subplots(figsize=(14, 1 + 0.55 * (len(df) + 2)))
    ax.axis("off")
    ax.set_title(title, fontsize=14, pad=12)

    table = ax.table(
        cellText=df.values,
        rowLabels=df.index.tolist(),
        colLabels=df.columns.tolist(),
        loc="center",
        cellLoc="center",
        colLoc="center",
    )
    table.auto_set_font_size(False)
    table.set_fontsize(font_size)
    table.scale(1, 1.25)

    plt.tight_layout()
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close(fig)


def compute_trades(close: pd.Series, signal: pd.Series):
    sig = signal.fillna(0).astype(int)
    in_pos = False
    entry_price = None
    trades = []

    for dt, s in sig.items():
        if not in_pos and s == 1:
            in_pos = True
            entry_price = float(close.loc[dt])
        elif in_pos and s == 0:
            exit_price = float(close.loc[dt])
            pnl = (exit_price - entry_price) / entry_price
            trades.append((dt, pnl * 100.0))
            in_pos = False
            entry_price = None

    if in_pos and entry_price is not None:
        dt = sig.index[-1]
        exit_price = float(close.iloc[-1])
        pnl = (exit_price - entry_price) / entry_price
        trades.append((dt, pnl * 100.0))

    return trades


def backtest_equity(close: pd.Series, signal: pd.Series, initial=10000.0):
    s = signal.fillna(0).astype(int)
    ret = close.pct_change().fillna(0.0)
    pos = s.shift(1).fillna(0)           # next-day execution (no look-ahead)
    strat_ret = pos * ret
    equity = (1 + strat_ret).cumprod() * initial
    bh_equity = (1 + ret).cumprod() * initial
    return equity, bh_equity, strat_ret


def plot_strategy_3panel(df: pd.DataFrame, signal_col: str, title: str, out_path: str):
    d = df.copy().sort_index()
    close = d["Close"]
    sig = d[signal_col].fillna(0).astype(int)

    equity, bh_equity, _ = backtest_equity(close, sig, initial=INITIAL_CAPITAL)
    trades = compute_trades(close, sig)

    fig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)

    axes[0].plot(close.index, close.values, label="Close", color="tab:blue")
    buy_idx = (sig.diff() == 1)
    sell_idx = (sig.diff() == -1)
    axes[0].scatter(close.index[buy_idx], close[buy_idx], marker="^", color="green", label="Buy", s=30)
    axes[0].scatter(close.index[sell_idx], close[sell_idx], marker="v", color="red", label="Sell", s=30)
    axes[0].set_title("Orders")
    axes[0].set_ylabel("Price")
    axes[0].legend(loc="upper left")

    if trades:
        t_dates = [t[0] for t in trades]
        t_pnl = [t[1] for t in trades]
        colors = ["green" if p >= 0 else "red" for p in t_pnl]
        axes[1].scatter(t_dates, t_pnl, c=colors, s=30)
    axes[1].axhline(0, linestyle="--", color="gray")
    axes[1].set_title("Trade PnL")
    axes[1].set_ylabel("Trade PnL (%)")

    axes[2].plot(equity.index, equity.values / INITIAL_CAPITAL, label="Value", color="purple")
    axes[2].plot(bh_equity.index, bh_equity.values / INITIAL_CAPITAL, label="Benchmark", color="black", alpha=0.6)
    axes[2].fill_between(equity.index, 1.0, equity.values / INITIAL_CAPITAL, color="green", alpha=0.15)
    axes[2].set_title("Cumulative Returns")
    axes[2].set_ylabel("Cumulative returns (x)")
    axes[2].legend(loc="upper left")

    fig.suptitle(title, fontsize=14)
    plt.tight_layout()
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close(fig)


# ================= CSV LOADER (supports your AAPL format) =================
def load_price_csv_any(symbol: str) -> pd.DataFrame:
    path = os.path.join(PRICES_DIR, f"{symbol}.csv")
    if not os.path.exists(path):
        raise FileNotFoundError(f"Missing file: {path}")

    # Standard
    try:
        df0 = pd.read_csv(path)
        if "Date" in df0.columns:
            df0["Date"] = pd.to_datetime(df0["Date"], errors="coerce")
            df0 = df0.dropna(subset=["Date"]).set_index("Date")
            need = ["Open", "High", "Low", "Close", "Volume"]
            if all(c in df0.columns for c in need):
                df0 = df0[need].apply(pd.to_numeric, errors="coerce").dropna()
                df0 = df0[~df0.index.duplicated(keep="last")].sort_index()
                return df0
    except Exception:
        pass

    # Multi-header
    df = pd.read_csv(path, header=[0, 1], index_col=0)
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)

    df.index = pd.to_datetime(df.index, errors="coerce")
    df = df[df.index.notna()]

    need = ["Open", "High", "Low", "Close", "Volume"]
    for c in need:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    df = df[need].dropna()
    df = df[~df.index.duplicated(keep="last")].sort_index()
    return df


# ================= INDICATORS =================
def add_indicators(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    c, h, l, v = df["Close"], df["High"], df["Low"], df["Volume"]

    df["SMA_20"] = c.rolling(20).mean()
    df["SMA_50"] = c.rolling(50).mean()

    d = c.diff()
    gain = d.clip(lower=0)
    loss = -d.clip(upper=0)
    rs = gain.rolling(14).mean() / loss.rolling(14).mean()
    df["RSI_14"] = 100 - (100 / (1 + rs))

    ema12 = c.ewm(span=12, adjust=False).mean()
    ema26 = c.ewm(span=26, adjust=False).mean()
    df["MACD"] = ema12 - ema26
    df["MACD_signal"] = df["MACD"].ewm(span=9, adjust=False).mean()

    mid = c.rolling(20).mean()
    std = c.rolling(20).std()
    df["BB_mid"] = mid
    df["BB_upper"] = mid + 2 * std
    df["BB_lower"] = mid - 2 * std

    obv = [0]
    for i in range(1, len(c)):
        if c.iloc[i] > c.iloc[i - 1]:
            obv.append(obv[-1] + v.iloc[i])
        elif c.iloc[i] < c.iloc[i - 1]:
            obv.append(obv[-1] - v.iloc[i])
        else:
            obv.append(obv[-1])
    df["OBV"] = obv
    df["OBV_MA20"] = df["OBV"].rolling(20).mean()

    conv = (h.rolling(9).max() + l.rolling(9).min()) / 2
    base = (h.rolling(26).max() + l.rolling(26).min()) / 2
    df["ICH_conv"] = conv
    df["ICH_base"] = base
    df["ICH_span_a"] = (conv + base) / 2
    df["ICH_span_b"] = (h.rolling(52).max() + l.rolling(52).min()) / 2

    df.dropna(inplace=True)
    return df.sort_index()


# ================= TRADITIONAL SIGNALS (FIXED EXIT LOGIC) =================
def sig_ma(df):
    return (df["SMA_20"] > df["SMA_50"]).astype(int)

def sig_macd(df):
    return (df["MACD"] > df["MACD_signal"]).astype(int)

def sig_obv(df):
    return (df["OBV"] > df["OBV_MA20"]).astype(int)

def sig_rsi(df):
    sig = pd.Series(np.nan, index=df.index, dtype=float)
    sig[df["RSI_14"] < 30] = 1
    sig[df["RSI_14"] > 70] = 0
    return sig.ffill().fillna(0).astype(int)

def sig_bb(df):
    sig = pd.Series(np.nan, index=df.index, dtype=float)
    sig[df["Close"] < df["BB_lower"]] = 1
    sig[df["Close"] > df["BB_mid"]] = 0
    return sig.ffill().fillna(0).astype(int)

def sig_ich(df):
    top = df[["ICH_span_a", "ICH_span_b"]].max(axis=1)
    bot = df[["ICH_span_a", "ICH_span_b"]].min(axis=1)
    sig = pd.Series(np.nan, index=df.index, dtype=float)
    sig[(df["Close"] > top) & (df["ICH_conv"] > df["ICH_base"])] = 1
    sig[(df["Close"] < bot) & (df["ICH_conv"] < df["ICH_base"])] = 0
    return sig.ffill().fillna(0).astype(int)


# ================= ML ALGORITHM SIGNAL (RISK FILTER) =================
def sig_algo_risk_filter(df: pd.DataFrame) -> pd.Series:
    d = df.copy().sort_index()

    # 5-day horizon label
    d["future_ret"] = d["Close"].shift(-LABEL_HORIZON_DAYS) / d["Close"] - 1
    d["y"] = (d["future_ret"] > LABEL_RET_THR).astype(int)

    feat_cols = [
        "SMA_20", "SMA_50", "RSI_14",
        "MACD", "MACD_signal",
        "BB_mid", "BB_upper", "BB_lower",
        "OBV", "OBV_MA20",
        "ICH_conv", "ICH_base", "ICH_span_a", "ICH_span_b",
    ]
    d = d.dropna(subset=feat_cols + ["y"]).copy()

    out = pd.Series(START_IN_MARKET, index=df.index, dtype=int)
    if len(d) < (WARMUP_BARS + 50):
        return out

    X = d[feat_cols].values.astype(np.float32)
    y = d["y"].values.astype(int)
    idx = d.index
    n = len(d)

    def make_model():
        if HGB_AVAILABLE:
            return HistGradientBoostingClassifier(max_depth=3, learning_rate=0.05, max_iter=250, random_state=42)
        return GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=3, random_state=42)

    i = WARMUP_BARS
    prev_pos = START_IN_MARKET

    while i < n - 1:
        model = make_model()
        model.fit(X[:i], y[:i])

        j = min(i + RETRAIN_EVERY, n)
        probs = model.predict_proba(X[i:j])[:, 1]

        block = []
        for p in probs:
            if p >= ENTER_THR:
                prev_pos = 1
            elif p <= EXIT_THR:
                prev_pos = 0
            block.append(prev_pos)

        out.loc[idx[i:j]] = np.array(block, dtype=int)
        i = j

    return out


def total_return(close: pd.Series, signal: pd.Series):
    equity, _, _ = backtest_equity(close.sort_index(), signal.sort_index(), initial=INITIAL_CAPITAL)
    end_val = float(equity.iloc[-1])
    ret_pct = (end_val / INITIAL_CAPITAL - 1) * 100.0
    return float(ret_pct), end_val


def main():
    df0 = load_price_csv_any(SYMBOL)
    df0 = df0[(df0.index >= pd.to_datetime(START_DISPLAY)) & (df0.index < pd.to_datetime(END_EXCL))].copy()
    df0 = df0.sort_index()

    df = add_indicators(df0)

    # -------- BUY & HOLD sanity check (ADDED) --------
    bh_end = INITIAL_CAPITAL * (df["Close"].iloc[-1] / df["Close"].iloc[0])
    bh_ret = (bh_end / INITIAL_CAPITAL - 1) * 100
    # -----------------------------------------------

    df["sig_ma"] = sig_ma(df)
    df["sig_rsi"] = sig_rsi(df)
    df["sig_macd"] = sig_macd(df)
    df["sig_bb"] = sig_bb(df)
    df["sig_obv"] = sig_obv(df)
    df["sig_ich"] = sig_ich(df)
    df["sig_algo"] = sig_algo_risk_filter(df)

    # Figures
    plot_strategy_3panel(df, "sig_ma",   f"FIGURE 5. MA20_MA50 ({SYMBOL})",             os.path.join(OUTPUT_DIR, "FIGURE_5_MA20_MA50.png"))
    plot_strategy_3panel(df, "sig_rsi",  f"FIGURE 6. RSI ({SYMBOL})",                   os.path.join(OUTPUT_DIR, "FIGURE_6_RSI.png"))
    plot_strategy_3panel(df, "sig_macd", f"FIGURE 7. MACD ({SYMBOL})",                  os.path.join(OUTPUT_DIR, "FIGURE_7_MACD.png"))
    plot_strategy_3panel(df, "sig_bb",   f"FIGURE 8. Bollinger Bands ({SYMBOL})",       os.path.join(OUTPUT_DIR, "FIGURE_8_BB.png"))
    plot_strategy_3panel(df, "sig_obv",  f"FIGURE 9. OBV ({SYMBOL})",                   os.path.join(OUTPUT_DIR, "FIGURE_9_OBV.png"))
    plot_strategy_3panel(df, "sig_ich",  f"FIGURE 10. Ichimoku ({SYMBOL})",             os.path.join(OUTPUT_DIR, "FIGURE_10_ICHIMOKU.png"))
    plot_strategy_3panel(df, "sig_algo", f"FIGURE 11. Approached Algorithm ({SYMBOL})", os.path.join(OUTPUT_DIR, "FIGURE_11_ALGORITHM.png"))

    # Table 8
    r_ma, end_ma = total_return(df["Close"], df["sig_ma"])
    r_rsi, end_rsi = total_return(df["Close"], df["sig_rsi"])
    r_macd, end_macd = total_return(df["Close"], df["sig_macd"])
    r_bb, end_bb = total_return(df["Close"], df["sig_bb"])
    r_obv, end_obv = total_return(df["Close"], df["sig_obv"])
    r_ich, end_ich = total_return(df["Close"], df["sig_ich"])
    r_algo, end_algo = total_return(df["Close"], df["sig_algo"])

    # ===== FINAL SUMMARY (for screenshot / report) =====
    print("\nFINAL SUMMARY")
    print("Period:", START_DISPLAY, "to", END_DISPLAY)
    print("Buy&Hold End Value:", int(bh_end))
    print("Buy&Hold Return %:", round(bh_ret, 2))

    print("Algorithm End Value:", int(end_algo))
    print("Algorithm Return %:", round(r_algo, 2))

    print("Algo in-market %:", round(df["sig_algo"].mean() * 100, 2))
    print("Algo trade count:", int((df["sig_algo"].diff().abs() == 1).sum() / 2))
    print("ENTER_THR:", ENTER_THR, "EXIT_THR:", EXIT_THR)
# ================================================

    table8 = pd.DataFrame({
        "MA20/MA50": [START_DISPLAY, END_DISPLAY, int(INITIAL_CAPITAL), int(end_ma), f"{r_ma:.0f}%"],
        "RSI":       [START_DISPLAY, END_DISPLAY, int(INITIAL_CAPITAL), int(end_rsi), f"{r_rsi:.0f}%"],
        "MACD":      [START_DISPLAY, END_DISPLAY, int(INITIAL_CAPITAL), int(end_macd), f"{r_macd:.0f}%"],
        "Bollinger": [START_DISPLAY, END_DISPLAY, int(INITIAL_CAPITAL), int(end_bb), f"{r_bb:.0f}%"],
        "OBV":       [START_DISPLAY, END_DISPLAY, int(INITIAL_CAPITAL), int(end_obv), f"{r_obv:.0f}%"],
        "Ichimoku":  [START_DISPLAY, END_DISPLAY, int(INITIAL_CAPITAL), int(end_ich), f"{r_ich:.0f}%"],
        "Algorithm": [START_DISPLAY, END_DISPLAY, int(INITIAL_CAPITAL), int(end_algo), f"{r_algo:.0f}%"],
    }, index=["Start", "End", "Start Value USD", "End Value USD", "Total Return"])

    save_table_png(table8, f"TABLE 8. Total return comparison ({SYMBOL})", os.path.join(OUTPUT_DIR, "TABLE_8.png"), font_size=10)
    print("Saved outputs/TABLE_8.png")


if __name__ == "__main__":
    main()


##indicators.py##

import numpy as np
import pandas as pd

def _kama(series: pd.Series, n=10) -> pd.Series:
    return series.ewm(span=n, adjust=False).mean()

def add_indicators_table3(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    c, h, l, v = df["Close"], df["High"], df["Low"], df["Volume"]

    delta = c.diff()
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)

    rs = gain.rolling(14).mean() / loss.rolling(14).mean()
    df["RSI"] = 100 - (100 / (1 + rs))

    low14 = l.rolling(14).min()
    high14 = h.rolling(14).max()
    df["STOCH_K"] = 100 * (c - low14) / (high14 - low14)

    tp = (h + l + c) / 3
    sma_tp = tp.rolling(20).mean()
    md = (tp - sma_tp).abs().rolling(20).mean()
    df["CCI"] = (tp - sma_tp) / (0.015 * md)

    sum_up = gain.rolling(14).sum()
    sum_down = loss.rolling(14).sum()
    df["CMO"] = 100 * (sum_up - sum_down) / (sum_up + sum_down)

    roc11 = c.pct_change(11) * 100
    roc14 = c.pct_change(14) * 100
    roc_sum = roc11 + roc14
    weights = np.arange(1, 11)
    df["COPP"] = roc_sum.rolling(10).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)

    df["EMA_20"] = c.ewm(span=20, adjust=False).mean()
    df["SMA_20"] = c.rolling(20).mean()
    df["SMA_50"] = c.rolling(50).mean()

    ema12 = c.ewm(span=12, adjust=False).mean()
    ema26 = c.ewm(span=26, adjust=False).mean()
    df["MACD"] = ema12 - ema26
    df["PPO"] = (df["MACD"] / ema26) * 100

    df["KAMA_10"] = _kama(c, n=10)
    df["VAMA_20"] = (c * v).rolling(20).sum() / v.rolling(20).sum()
    df["TRIMA_20"] = c.rolling(20).mean().rolling(20).mean()

    conv = (h.rolling(9).max() + l.rolling(9).min()) / 2
    base = (h.rolling(26).max() + l.rolling(26).min()) / 2
    df["ICHIMOKU_conv"] = conv
    df["ICHIMOKU_base"] = base
    df["ICHIMOKU_span_a"] = (conv + base) / 2
    df["ICHIMOKU_span_b"] = (h.rolling(52).max() + l.rolling(52).min()) / 2

    mid = c.rolling(20).mean()
    std = c.rolling(20).std()
    df["BB_upper"] = mid + 2 * std

    obv = [0]
    for i in range(1, len(c)):
        if c.iloc[i] > c.iloc[i - 1]:
            obv.append(obv[-1] + v.iloc[i])
        elif c.iloc[i] < c.iloc[i - 1]:
            obv.append(obv[-1] - v.iloc[i])
        else:
            obv.append(obv[-1])
    df["OBV"] = obv

    tp2 = (h + l + c) / 3
    rmf = tp2 * v
    prev_tp = tp2.shift(1)
    pos = rmf.where(tp2 > prev_tp, 0.0)
    neg = rmf.where(tp2 < prev_tp, 0.0)
    mfr = pos.rolling(14).sum() / neg.rolling(14).sum().abs()
    df["MFI_14"] = 100 - (100 / (1 + mfr))

    df["52w_high"] = c.rolling(252).max()

    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(inplace=True)
    return df

def compute_us_score(last_row: pd.Series) -> int:
    score = 0
    if last_row["Close"] < last_row["SMA_20"]:
        score += 1
    if last_row["Close"] < last_row["SMA_50"]:
        score += 1
    if last_row["RSI"] < 40:
        score += 1
    if last_row["52w_high"] > 0:
        disc = 1 - last_row["Close"] / last_row["52w_high"]
        if disc > 0.20:
            score += 1
    return score


##main.py##
from config import load_sp500_tickers, OUTPUT_DIR, HORIZON_DAYS
from dataset_builder import build_timeseries_dataset, build_us_timeseries_dataset
from feature_selection import paper_feature_select
from model_assortment import run_assortment
from report_paperstyle import save_table_png, plot_radar_from_metrics


def main():
    tickers = load_sp500_tickers()
    print("Tickers loaded:", len(tickers))

    EVAL_MODE = "REALISTIC"   # or "PAPER_US"

    if EVAL_MODE == "REALISTIC":
        H = HORIZON_DAYS
        df_ts = build_timeseries_dataset(
            tickers,
            horizon_days=H,
            offline_only=True,
            save_file=True,     # outputs/timeseries_dataset.csv
            add_regime=True
        )
        print("Time-series dataset rows:", len(df_ts))
        if df_ts.empty:
            print("No data generated.")
            return

        df_final, keep, missing = paper_feature_select(df_ts)
        print("Features kept:", keep)
        print("Missing:", missing)

        best_name, model_path, metrics = run_assortment(
            df_final,
            split_strategy="paper_holdout",
            train_end_date="2021-12-31",
            test_start_date="2022-01-01",
            test_end_date="2022-12-31",
            horizon_days=H,
            apply_smote_on_train=False,
            use_sample_weight=True,
            sort_by="BalancedAcc",
            save_csv=False
        )

    else:
        H = 0
        df_us = build_us_timeseries_dataset(
            tickers,
            offline_only=True,
            us_threshold=3,
            save_file=True,     # outputs/us_timeseries_dataset.csv
            add_regime=True
        )
        print("US dataset rows:", len(df_us))
        if df_us.empty:
            print("No data generated.")
            return

        df_final, keep, missing = paper_feature_select(df_us)
        print("Features kept:", keep)
        print("Missing:", missing)

        best_name, model_path, metrics = run_assortment(
            df_final,
            split_strategy="paper_holdout",
            train_end_date="2021-12-31",
            test_start_date="2022-01-01",
            test_end_date="2022-12-31",
            horizon_days=H,
            apply_smote_on_train=True,
            use_sample_weight=False,
            sort_by="BalancedAcc",
            save_csv=False
        )

    table7 = metrics[["Model","Accuracy","Precision","Recall","F1"]].copy()

    save_table_png(
        table7.set_index("Model").round(3),
        title="TABLE 7. Machine learning test performance details.",
        out_path=f"{OUTPUT_DIR}/TABLE_7.png",
        font_size=10
    )

    plot_radar_from_metrics(
        metrics_df=table7.round(3),
        out_path=f"{OUTPUT_DIR}/FIGURE_4_radar.png"
    )

    print("Saved: outputs/TABLE_7.png and outputs/FIGURE_4_radar.png")
    print("Best model:", best_name)
    print("Model file:", model_path)
    print("Next: run signals.py using outputs/timeseries_dataset.csv to keep symbol/date.")


if __name__ == "__main__":
    main()


##model_assortment.py##

import os
import time
import json
import numpy as np
import pandas as pd

from collections import Counter
from typing import Tuple, Dict, List, Optional

from sklearn.base import clone
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

from imblearn.over_sampling import SMOTE
import joblib

from config import OUTPUT_DIR


def get_models():
    return {
        "GBM": GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=3,
            subsample=0.8,
            random_state=42,
        ),
        "SVM": Pipeline([
            ("scaler", StandardScaler()),
            ("m", LinearSVC(class_weight="balanced", random_state=42, max_iter=5000))
        ]),
        "RandomForest": RandomForestClassifier(
            n_estimators=400,
            max_depth=10,
            min_samples_leaf=5,
            class_weight="balanced_subsample",
            random_state=42,
            n_jobs=-1,
        ),
        "LogisticRegression": Pipeline([
            ("scaler", StandardScaler()),
            ("m", LogisticRegression(max_iter=2000, class_weight="balanced", n_jobs=-1))
        ]),
        "NeuralNetwork": Pipeline([
            ("scaler", StandardScaler()),
            ("m", MLPClassifier(
                hidden_layer_sizes=(64, 32),
                max_iter=300,
                early_stopping=True,
                validation_fraction=0.2,
                n_iter_no_change=15,
                random_state=42
            ))
        ]),
        "DecisionTree": DecisionTreeClassifier(
            max_depth=6,
            min_samples_leaf=10,
            class_weight="balanced",
            random_state=42
        ),
        "KNN": Pipeline([
            ("scaler", StandardScaler()),
            ("m", KNeighborsClassifier(n_neighbors=7, n_jobs=-1))
        ]),
        "GaussianNB": Pipeline([
            ("scaler", StandardScaler()),
            ("m", GaussianNB())
        ]),
    }


def _median_impute(train_df: pd.DataFrame, test_df: pd.DataFrame, feature_cols: List[str]):
    med = train_df[feature_cols].median(numeric_only=True)
    Xtr = train_df[feature_cols].fillna(med).values
    Xte = test_df[feature_cols].fillna(med).values
    return Xtr, Xte


def _smote_train_only(Xtr: np.ndarray, ytr: np.ndarray):
    dist = Counter(ytr)
    if len(dist) < 2:
        return Xtr, ytr
    minority = min(dist.values())
    if minority < 2:
        return Xtr, ytr
    k = min(5, minority - 1)
    if k < 1:
        return Xtr, ytr
    sm = SMOTE(random_state=42, k_neighbors=k)
    return sm.fit_resample(Xtr, ytr)


def _purge_last_horizon_per_symbol(train_df: pd.DataFrame, horizon_days: int) -> pd.DataFrame:
    if horizon_days <= 0:
        return train_df
    t = train_df.copy()
    t["date"] = pd.to_datetime(t["date"], errors="coerce")
    t = t.dropna(subset=["date"]).sort_values(["symbol", "date"]).reset_index(drop=True)
    pos = t.groupby("symbol").cumcount()
    size = t.groupby("symbol")["symbol"].transform("size")
    keep = pos < (size - horizon_days)
    return t.loc[keep].copy()


def _prepare_features(df: pd.DataFrame):
    ignore = {"label", "symbol", "date", "US_Score", "Close", "Total_Trend"}
    feature_cols = [c for c in df.columns if c not in ignore]
    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]
    if not feature_cols:
        raise RuntimeError("No numeric features found after filtering.")
    return feature_cols


def _paper_holdout_split(df: pd.DataFrame, train_end_date: str, test_start_date: str, test_end_date: str):
    d = df.copy()
    d["date"] = pd.to_datetime(d["date"], errors="coerce")
    d = d.dropna(subset=["date"]).sort_values("date")

    tr_end = pd.to_datetime(train_end_date)
    te_start = pd.to_datetime(test_start_date)
    te_end = pd.to_datetime(test_end_date)

    train_df = d[d["date"] <= tr_end].copy()
    test_df = d[(d["date"] >= te_start) & (d["date"] <= te_end)].copy()
    return train_df, test_df


def _class_weights_as_sample_weight(y: np.ndarray) -> np.ndarray:
    dist = Counter(y)
    n = len(y)
    w = {int(cls): n / (len(dist) * cnt) for cls, cnt in dist.items()}
    return np.array([w[int(v)] for v in y], dtype=float)


def _fit_try_sample_weight(model, X, y, sw):
    try:
        if isinstance(model, Pipeline):
            return model.fit(X, y, **{"m__sample_weight": sw})
        return model.fit(X, y, sample_weight=sw)
    except TypeError:
        return model.fit(X, y)


def run_assortment(
    df: pd.DataFrame,
    test_size: float = 0.3,
    apply_smote_on_train: bool = True,
    use_sample_weight: bool = True,
    split_strategy: str = "paper_holdout",
    train_end_date: str = "2021-12-31",
    test_start_date: str = "2022-01-01",
    test_end_date: str = "2022-12-31",
    horizon_days: int = 5,
    knn_max_train: int = 20000,
    sort_by: str = "BalancedAcc",
    save_csv: bool = False,
) -> Tuple[str, str, pd.DataFrame]:

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    feature_cols = _prepare_features(df)
    models = get_models()

    if split_strategy == "paper_holdout":
        train_df, test_df = _paper_holdout_split(df, train_end_date, test_start_date, test_end_date)
        train_df = _purge_last_horizon_per_symbol(train_df, horizon_days=horizon_days)
    else:
        train_df, test_df = train_test_split(df, test_size=test_size, stratify=df["label"], random_state=42)

    if train_df.empty or test_df.empty:
        raise RuntimeError("Train/Test empty. Check dates/data range.")

    ytr = train_df["label"].values
    yte = test_df["label"].values
    print("Train distribution:", Counter(ytr))
    print("Test  distribution:", Counter(yte))

    Xtr, Xte = _median_impute(train_df, test_df, feature_cols)

    if apply_smote_on_train:
        Xtr_fit, ytr_fit = _smote_train_only(Xtr, ytr)
        sw = None
    else:
        Xtr_fit, ytr_fit = Xtr, ytr
        sw = _class_weights_as_sample_weight(ytr_fit) if use_sample_weight else None

    rows = []
    for name, base_model in models.items():
        t0 = time.time()

        X_use, y_use = Xtr_fit, ytr_fit
        sw_use = sw

        if name == "KNN" and len(ytr_fit) > knn_max_train:
            idx = np.random.default_rng(42).choice(len(ytr_fit), size=knn_max_train, replace=False)
            X_use, y_use = Xtr_fit[idx], ytr_fit[idx]
            sw_use = None

        model = clone(base_model)
        if sw_use is not None:
            _fit_try_sample_weight(model, X_use, y_use, sw_use)
        else:
            model.fit(X_use, y_use)

        pred = model.predict(Xte)
        tn, fp, fn, tp = confusion_matrix(yte, pred, labels=[0, 1]).ravel()

        acc = accuracy_score(yte, pred)
        prec = precision_score(yte, pred, pos_label=1, zero_division=0)
        rec = recall_score(yte, pred, pos_label=1, zero_division=0)
        f1 = f1_score(yte, pred, pos_label=1, zero_division=0)

        specificity = (tn / (tn + fp)) if (tn + fp) > 0 else 0.0
        balanced = 0.5 * (rec + specificity)
        pred_pos_rate = float(np.mean(pred))

        rows.append([name, acc, prec, rec, f1, specificity, balanced, pred_pos_rate])
        print(f"[DONE] {name:16s} Acc={acc:.3f} F1={f1:.3f} BalAcc={balanced:.3f} time={time.time()-t0:.1f}s")

    metrics = pd.DataFrame(rows, columns=["Model","Accuracy","Precision","Recall","F1","Specificity","BalancedAcc","PredPosRate"])
    if sort_by not in metrics.columns:
        sort_by = "BalancedAcc"
    metrics = metrics.sort_values(sort_by, ascending=False).reset_index(drop=True)

    if save_csv:
        metrics.to_csv(f"{OUTPUT_DIR}/table7_model_metrics.csv", index=False)

    best_name = str(metrics.iloc[0]["Model"])
    best_model = clone(models[best_name])

    # Fit best model on full data (deployment bundle)
    X_full = df[feature_cols].copy().fillna(df[feature_cols].median(numeric_only=True)).values
    y_full = df["label"].values
    if (not apply_smote_on_train) and use_sample_weight:
        sw_full = _class_weights_as_sample_weight(y_full)
        _fit_try_sample_weight(best_model, X_full, y_full, sw_full)
    else:
        best_model.fit(X_full, y_full)

    bundle = {
        "model": best_model,
        "feature_cols": feature_cols,
        "best_name": best_name,
    }

    model_path = f"{OUTPUT_DIR}/best_model_{best_name}.joblib"
    joblib.dump(bundle, model_path)

    with open(f"{OUTPUT_DIR}/best_model_{best_name}_meta.json", "w") as f:
        json.dump({"best_name": best_name, "feature_cols": feature_cols}, f, indent=2)

    print("\nSaved model bundle:", model_path)
    return best_name, model_path, metrics


##recommend_buy.py##
import pandas as pd

TOP_N = 10
SIGNALS_CSV = "outputs/trading_signals_best_model.csv"
SP500_CSV = "SP500.csv"

df = pd.read_csv(SIGNALS_CSV)
df["date"] = pd.to_datetime(df["date"], errors="coerce")
df = df.dropna(subset=["date", "symbol"])
df["symbol"] = df["symbol"].astype(str).str.strip().str.upper().str.replace(".", "-", regex=False)

latest_date = df["date"].max()
snap = df[df["date"] == latest_date].copy()
top = snap.sort_values("prob_buy", ascending=False).head(TOP_N).copy()
top["final_action"] = "BUY (Top-N)"

sp = pd.read_csv(SP500_CSV)
sp = sp.rename(columns={"Symbol": "symbol", "Name": "company"})
sp["symbol"] = sp["symbol"].astype(str).str.strip().str.upper().str.replace(".", "-", regex=False)

top = top.merge(sp[["symbol", "company", "Sector"]], on="symbol", how="left")

out_path = "outputs/recommended_buy_list.csv"
top.to_csv(out_path, index=False)

print("Latest date:", latest_date.date())
print(top[["symbol", "company", "Sector", "date", "prob_buy", "final_action"]])
print("Saved:", out_path)


##report_paperstyle.py##
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def save_table_png(df: pd.DataFrame, title: str, out_path: str, font_size: int = 10):
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    fig, ax = plt.subplots(figsize=(10, 2.8))
    ax.axis("off")
    tbl = ax.table(cellText=df.values, colLabels=df.columns, rowLabels=df.index, loc="center")
    tbl.auto_set_font_size(False)
    tbl.set_fontsize(font_size)
    tbl.scale(1, 1.3)
    ax.set_title(title, fontsize=12, pad=12)
    fig.set_constrained_layout(True)
    plt.savefig(out_path, dpi=200, bbox_inches="tight")
    plt.close(fig)

def plot_radar_from_metrics(metrics_df: pd.DataFrame, out_path: str):
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    cols = ["Accuracy","Precision","Recall","F1"]

    df = metrics_df.copy()
    if "Model" in df.columns:
        df = df.set_index("Model")
    df = df[cols]

    labels = cols
    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()
    angles += angles[:1]

    fig = plt.figure(figsize=(7, 7))
    ax = plt.subplot(111, polar=True)

    for name, row in df.iterrows():
        vals = row.values.astype(float).tolist()
        vals += vals[:1]
        ax.plot(angles, vals, linewidth=1.5, label=str(name))
        ax.fill(angles, vals, alpha=0.06)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels)
    ax.set_ylim(0, 1)
    ax.set_title("FIGURE 4. Radar chart of ML test performance.", pad=20)
    ax.legend(loc="upper right", bbox_to_anchor=(1.25, 1.15))
    fig.set_constrained_layout(True)
    plt.savefig(out_path, dpi=200, bbox_inches="tight")
    plt.close(fig)


##signals.py##

import os
import glob
import numpy as np
import pandas as pd
import joblib

from config import BUY_THR, SELL_THR, OUTPUT_DIR


def _sigmoid(x: np.ndarray) -> np.ndarray:
    return 1.0 / (1.0 + np.exp(-x))


def generate_signals(dataset_csv: str, model_path: str) -> pd.DataFrame:
    df = pd.read_csv(dataset_csv)

    # STRICT: we must have symbol/date to generate company-wise outputs
    if "symbol" not in df.columns or "date" not in df.columns:
        raise ValueError(
            f"Input dataset '{dataset_csv}' does NOT contain symbol/date. "
            "Use outputs/timeseries_dataset.csv (or outputs/us_timeseries_dataset.csv)."
        )

    bundle = joblib.load(model_path)
    model = bundle["model"]
    feature_cols = bundle["feature_cols"]

    # derive MA20_MA50 if training expects it
    if "MA20_MA50" in feature_cols and "MA20_MA50" not in df.columns:
        if "SMA_20" in df.columns and "SMA_50" in df.columns:
            df["MA20_MA50"] = (df["SMA_20"] - df["SMA_50"]).astype(float)
        else:
            raise ValueError("MA20_MA50 needed but SMA_20/SMA_50 not found in dataset.")

    # ensure required features exist now
    missing = [c for c in feature_cols if c not in df.columns]
    if missing:
        raise ValueError(f"Dataset missing required feature columns: {missing}")

    X = df[feature_cols].copy()
    X = X.fillna(X.median(numeric_only=True)).values

    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(X)[:, 1]
    elif hasattr(model, "decision_function"):
        proba = _sigmoid(model.decision_function(X))
    else:
        proba = model.predict(X).astype(float)

    signal = np.where(proba >= BUY_THR, 1,
             np.where(proba <= SELL_THR, -1, 0))

    out = df.copy()            # IMPORTANT: keeps symbol/date/Close/etc.
    out["prob_buy"] = proba
    out["signal"] = signal

    out_path = os.path.join(OUTPUT_DIR, "trading_signals_best_model.csv")
    out.to_csv(out_path, index=False)
    print("Saved:", out_path)
    return out


if __name__ == "__main__":
    # 1) pick latest best_model_*.joblib automatically
    candidates = sorted(glob.glob(os.path.join(OUTPUT_DIR, "best_model_*.joblib")))
    if not candidates:
        raise FileNotFoundError(f"No best_model_*.joblib found in {OUTPUT_DIR}. Run main.py first.")
    model_path = candidates[-1]
    print("Using model:", model_path)

    # 2) ALWAYS use full dataset with symbol/date
    dataset_path = os.path.join(OUTPUT_DIR, "timeseries_dataset.csv")
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(
            f"{dataset_path} not found. Run main.py with save_file=True to generate it."
        )

    # show columns to confirm
    tmp = pd.read_csv(dataset_path, nrows=2)
    print("Dataset columns (sample):", tmp.columns.tolist())

    # 3) generate signals
    generate_signals(dataset_path, model_path)



##table5_paperstyle.py##

import os
import ssl
import time
import numpy as np
import pandas as pd
import yfinance as yf

from report_paperstyle import save_table_png

# SSL workaround
ssl._create_default_https_context = ssl._create_unverified_context

DATA_DIR = "data"
PRICES_DIR = os.path.join(DATA_DIR, "prices")
OUTPUT_DIR = "outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

OFFLINE_ONLY = True   # keep True for your environment
EPS = 0.001           # neutral threshold

BACKWARD_START = "2022-01-01"
BACKWARD_END   = "2022-12-31"
FORWARD_START  = "2023-01-01"
FORWARD_END    = "2023-12-31"


# ---- CSV loader (supports normal + yfinance multi-header) ----
def load_price_csv_any(symbol: str):
    path = os.path.join(PRICES_DIR, f"{symbol}.csv")
    if not os.path.exists(path):
        return None

    # Standard format
    try:
        df0 = pd.read_csv(path)
        if "Date" in df0.columns:
            df0["Date"] = pd.to_datetime(df0["Date"], errors="coerce")
            df0 = df0.dropna(subset=["Date"]).set_index("Date")
            needed = ["Open", "High", "Low", "Close", "Volume"]
            if all(c in df0.columns for c in needed):
                df0 = df0[needed].apply(pd.to_numeric, errors="coerce").dropna()
                return df0
    except Exception:
        pass

    # Multi-header format
    try:
        df = pd.read_csv(path, header=[0, 1], index_col=0)
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        df.index = pd.to_datetime(df.index, errors="coerce")
        df = df[df.index.notna()]
        needed = ["Open", "High", "Low", "Close", "Volume"]
        for c in needed:
            df[c] = pd.to_numeric(df[c], errors="coerce")
        df = df[needed].dropna()
        return df
    except Exception:
        return None


def yf_download_retry(symbol: str, start: str, end: str, retries: int = 3):
    try:
        import yfinance.shared as shared
        shared._DEFAULTS["timeout"] = 60
    except Exception:
        pass

    last = None
    for i in range(1, retries + 1):
        try:
            df = yf.download(symbol, start=start, end=end, auto_adjust=True, progress=False, threads=False)
            if df is not None and not df.empty:
                df = df[["Open", "High", "Low", "Close", "Volume"]].dropna()
                return df
            last = "empty"
        except Exception as e:
            last = str(e)
        print(f"[WARN] yfinance failed {symbol} ({i}/{retries}): {last}")
        time.sleep(1.5)
    return None


def get_close(symbol: str, start: str, end: str):
    df = load_price_csv_any(symbol)
    if df is not None and not df.empty:
        df = df[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]
        if not df.empty:
            return df["Close"]

    if OFFLINE_ONLY:
        return None

    df = yf_download_retry(symbol, start, end, retries=3)
    if df is None or df.empty:
        return None
    df = df[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]
    return df["Close"] if not df.empty else None


def period_return(symbol: str, start: str, end: str):
    s = get_close(symbol, start, end)
    if s is None or s.empty:
        return None
    # entry = first available
    entry = float(s.iloc[0])
    exitp = float(s.iloc[-1])
    return (exitp - entry) / entry


def compute_win_neutral_loss(selected: list[str], benchmark_symbol: str, start: str, end: str):
    bench = period_return(benchmark_symbol, start, end)
    if bench is None:
        # If benchmark missing, return NA safely
        return (0, 0, 0), None

    win = neu = loss = 0
    rets = []
    for sym in selected:
        r = period_return(sym, start, end)
        if r is None:
            continue
        rets.append(r)
        diff = r - bench
        if diff > EPS:
            win += 1
        elif diff < -EPS:
            loss += 1
        else:
            neu += 1

    # equal-weight cumulative return
    cum_ret = float(np.mean(rets)) if rets else 0.0
    return (win, neu, loss), cum_ret


def main():
    """
    Paper Table-5 uses these selected tickers (from your Figures 1-3):
      S&P500:   HES, ON, SLB, XOM  (4)
      NASDAQ100: CDNS, MNST, ODFL, ROST, VRTX (5)
      Dow: CVX, MCD, TRV (3)

    Benchmarks (ETF) (offline CSV recommended):
      SPY for S&P500, QQQ for NASDAQ100, DIA for Dow
    """
    indices = [
        ("S&P 500", 503, ["HES", "ON", "SLB", "XOM"], "SPY"),
        ("Nasdaq100", 100, ["CDNS", "MNST", "ODFL", "ROST", "VRTX"], "QQQ"),
        ("Dow Jones", 30, ["CVX", "MCD", "TRV"], "DIA"),
    ]

    rows = []
    for idx_name, total, selected, bench in indices:
        # Backward
        (w1, n1, l1), cr1 = compute_win_neutral_loss(selected, bench, BACKWARD_START, BACKWARD_END)
        # Forward
        (w2, n2, l2), cr2 = compute_win_neutral_loss(selected, bench, FORWARD_START, FORWARD_END)

        rows.append({
            "Index": idx_name,
            "Total": total,
            "Hit": len(selected),
            "Backward WIN:Neutral:LOSS": f"{w1}:{n1}:{l1}",
            "Backward Cumulative Return": "" if cr1 is None else f"{cr1*100:.2f}%",
            "Forward WIN:Neutral:LOSS": f"{w2}:{n2}:{l2}",
            "Forward Cumulative Return": "" if cr2 is None else f"{cr2*100:.2f}%",
        })

    df = pd.DataFrame(rows)
    save_table_png(
        df,
        title="TABLE 5. Backward test and forward test.",
        out_path=os.path.join(OUTPUT_DIR, "TABLE_5.png"),
        font_size=10
    )
    print("Saved outputs/TABLE_5.png")
    print("Note: For accurate win/loss vs benchmark, keep SPY.csv, QQQ.csv, DIA.csv in data/prices/ (offline).")


if __name__ == "__main__":
    main()



##table6_paperstyle.py##
import os
import ast
import ssl
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf

ssl._create_default_https_context = ssl._create_unverified_context
plt.style.use("seaborn-v0_8")

ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(ROOT_DIR, "data")
PRICES_DIR = os.path.join(DATA_DIR, "prices")
OUTPUT_DIR = os.path.join(ROOT_DIR, "outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# If your internet is bad, keep True (uses only local CSVs in data/prices/)
OFFLINE_ONLY = True

EVAL_DATES = [
    ("Return (%) on Dec 2021", "2021-12-31"),
    ("Return (%) on Dec 2022", "2022-12-31"),
    ("Return (%) on Dec 2023", "2023-12-31"),
    ("Return (%) on June 2024", "2024-06-30"),
]

# --- Hardcoded Table 6 recommended stocks (from your paper screenshots) ---
DCA_ROWS = [
    # No, DCA_date, Recommend_Stock
    (1,  "2020-01-01", ["AAPL","AMD","TSLA"]),
    (2,  "2020-02-01", ["AMD","NVDA","TSLA"]),
    (3,  "2020-03-01", []),
    (4,  "2020-04-01", ["AMD","MRNA","NVDA"]),
    (5,  "2020-05-01", ["AMD","MRNA","NVDA","TSLA"]),
    (6,  "2020-06-01", ["NVDA","TSLA"]),
    (7,  "2020-07-01", ["AAPL","AMD","MRNA","NVDA","PYPL","TSLA"]),
    (8,  "2020-08-01", ["AAPL","AMD","NOW","NVDA","PYPL","TSLA"]),
    (9,  "2020-09-01", ["NVDA","TSLA"]),
    (10, "2020-10-01", ["AAPL","CRM","NVDA","PYPL","TSLA"]),
    (11, "2020-11-01", ["AAPL","AMD","CRM","MRNA","NVDA","QCOM","TSLA"]),
    (12, "2020-12-01", ["AAPL","AMD","AVGO","CRWD","MRNA","NKE","PYPL","QCOM","TSLA","UBER"]),

    (13, "2021-01-01", ["AAPL","DD","ETSY","F","GM","GS","MU","PYPL","QCOM","TSLA","UBER"]),
    (14, "2021-02-01", ["DIS","GM","GOOG","GOOGL","LRCX","MRNA","MU","NVDA","PYPL","TSLA","UBER"]),
    (15, "2021-03-01", ["AAL","AMAT","AVGO","BA","BAC","C","CAT","CCL","CZR","DIS","F","GE","GM","GNRC","GS","MU","NXPI","PARA","UAL"]),
    (16, "2021-04-01", ["AMAT","BA","MRNA","MU","NVDA","PYPL","TSLA","UBER","WFC"]),
    (17, "2021-05-01", ["FCX","WFC"]),
    (18, "2021-06-01", ["F","GM","MRNA","NVDA"]),
    (19, "2021-07-01", ["MRNA"]),
    (20, "2021-08-01", ["WFC"]),
    (21, "2021-09-01", ["AMAT","GOOG","GOOGL","GS","MRNA","WFC"]),
    (22, "2021-10-01", ["F","GS","MS","WFC"]),
    (23, "2021-11-01", ["AMD","F","NVDA","TSLA","WFC"]),
    (24, "2021-12-01", ["EPAM","F","NVDA"]),

    (25, "2022-01-01", ["F"]),
    (26, "2022-02-01", []),
    (27, "2022-03-01", ["DVN","OXY"]),
    (28, "2022-04-01", ["OXY"]),
    (29, "2022-05-01", ["OXY"]),
    (30, "2022-06-01", ["OXY"]),
    (31, "2022-07-01", []),
    (32, "2022-08-01", ["OXY"]),
    (33, "2022-09-01", ["ENPH","OXY"]),
    (34, "2022-10-01", []),
    (35, "2022-11-01", ["OXY"]),
    (36, "2022-12-01", ["ENPH"]),

    (37, "2023-01-01", []),
    (38, "2023-02-01", []),
    (39, "2023-03-01", []),
    (40, "2023-04-01", ["NFLX","NVDA"]),
    (41, "2023-05-01", ["NVDA"]),
    (42, "2023-06-01", ["AMD","META","NFLX","NVDA"]),
    (43, "2023-07-01", ["AMD","CCL","META","NFLX","NVDA"]),
    (44, "2023-08-01", ["AVGO","META","NFLX","NVDA"]),
    (45, "2023-09-01", ["ADBE","META","NFLX","NVDA","ORCL"]),
    (46, "2023-10-01", ["META","TSLA"]),
    (47, "2023-11-01", ["META","NVDA"]),
    (48, "2023-12-01", ["META","NVDA"]),
]

# ---------------- helpers ----------------

def save_table_png(df: pd.DataFrame, title: str, out_path: str, font_size: int = 8):
    fig, ax = plt.subplots(figsize=(14, 1 + 0.45 * (len(df) + 2)))
    ax.axis("off")
    ax.set_title(title, fontsize=14, pad=12)

    table = ax.table(
        cellText=df.values,
        colLabels=df.columns,
        loc="center",
        cellLoc="center",
        colLoc="center",
    )
    table.auto_set_font_size(False)
    table.set_fontsize(font_size)
    table.scale(1, 1.2)

    plt.tight_layout()
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close(fig)


def load_price_csv_any(symbol: str):
    """
    Supports both standard and your multi-header format.
    data/prices/<SYMBOL>.csv
    """
    path = os.path.join(PRICES_DIR, f"{symbol}.csv")
    if not os.path.exists(path):
        return None

    # Standard
    try:
        df0 = pd.read_csv(path)
        if "Date" in df0.columns:
            df0["Date"] = pd.to_datetime(df0["Date"], errors="coerce")
            df0 = df0.dropna(subset=["Date"]).set_index("Date")
            needed = ["Open", "High", "Low", "Close", "Volume"]
            if all(c in df0.columns for c in needed):
                df0 = df0[needed].apply(pd.to_numeric, errors="coerce").dropna()
                return df0
    except Exception:
        pass

    # Multi-header
    try:
        df = pd.read_csv(path, header=[0, 1], index_col=0)
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        df.index = pd.to_datetime(df.index, errors="coerce")
        df = df[df.index.notna()]
        needed = ["Open", "High", "Low", "Close", "Volume"]
        for c in needed:
            df[c] = pd.to_numeric(df[c], errors="coerce")
        df = df[needed].dropna()
        return df
    except Exception:
        return None


def yf_download_retry(symbol: str, start: str, end: str, retries: int = 3):
    try:
        import yfinance.shared as shared
        shared._DEFAULTS["timeout"] = 60
    except Exception:
        pass

    last = None
    for i in range(1, retries + 1):
        try:
            df = yf.download(symbol, start=start, end=end, auto_adjust=True, progress=False, threads=False)
            if df is not None and not df.empty:
                df = df[["Open", "High", "Low", "Close", "Volume"]].dropna()
                return df
            last = "empty"
        except Exception as e:
            last = str(e)
        time.sleep(1.5)
    return None


def get_close(symbol: str, start: str, end: str):
    df = load_price_csv_any(symbol)
    if df is not None and not df.empty:
        df = df[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]
        if not df.empty:
            return df["Close"]

    if OFFLINE_ONLY:
        return None

    df = yf_download_retry(symbol, start, end, retries=3)
    if df is None or df.empty:
        return None
    df = df[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]
    return df["Close"] if not df.empty else None


def portfolio_return(tickers, start_date, end_date):
    if not tickers:
        return None

    rets = []
    for sym in tickers:
        s = get_close(sym, start_date, end_date)
        if s is None or s.empty:
            continue
        entry = float(s.iloc[0])
        exitp = float(s.iloc[-1])
        rets.append((exitp - entry) / entry)

    if not rets:
        return None
    return float(np.mean(rets)) * 100.0


def main():
    # build base table
    df = pd.DataFrame({
        "No": [r[0] for r in DCA_ROWS],
        "DCA date": [r[1] for r in DCA_ROWS],
        "Total": [len(r[2]) for r in DCA_ROWS],
        "Recommend Stock": [str(r[2]) for r in DCA_ROWS],
    })
    df["DCA date"] = pd.to_datetime(df["DCA date"])

    # compute returns columns (blank if prices missing)
    for col, ed in EVAL_DATES:
        vals = []
        for d, tickers in zip(df["DCA date"], [r[2] for r in DCA_ROWS]):
            if d > pd.to_datetime(ed):
                vals.append("")
                continue
            r = portfolio_return(tickers, d.strftime("%Y-%m-%d"), ed)
            vals.append("" if r is None else round(r, 2))
        df[col] = vals

    # summary row (mean of available)
    summary = {"No": "", "DCA date": "DCA Total Return Year to Date (%)", "Total": "", "Recommend Stock": ""}
    for col, _ in EVAL_DATES:
        v = pd.to_numeric(df[col], errors="coerce")
        summary[col] = round(v.dropna().mean(), 2) if v.notna().any() else ""
    df_out = pd.concat([df, pd.DataFrame([summary])], ignore_index=True)

    # split into two parts (like paper)
    part1 = df_out.iloc[:24].copy()
    part2 = df_out.iloc[24:].copy()

    p1 = os.path.join(OUTPUT_DIR, "TABLE_6_part1.png")
    p2 = os.path.join(OUTPUT_DIR, "TABLE_6_part2.png")

    save_table_png(part1, "TABLE 6. DCA performance and recommended stock overview (Part 1).", p1, font_size=8)
    save_table_png(part2, "TABLE 6. (Continued.) DCA performance and recommended stock overview (Part 2).", p2, font_size=8)

    print("Saved:", p1)
    print("Saved:", p2)
    print("Note: If returns are blank, add required ticker CSVs into data/prices/.")


if __name__ == "__main__":
    main()



##manuscript.tex##

\documentclass[journal]{IEEEtran}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{multirow}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\title{Machine Learning-Based Stock Market Prediction and Backtesting System}

\author{
Rohit~B~(22BCS175), Ayyanar~N~(22BCS156)\\
Department of Computer Science and Engineering, Mepco Schlenk Engineering College (Autonomous)\\
Guided by: Dr.~D.~Sasireka, Assistant Professor
}

\begin{document}
\maketitle

\begin{abstract}
Stock market decision-making is challenging due to noise, uncertainty, and changing market regimes. This manuscript presents an end-to-end machine learning (ML) system for stock price direction prediction and undervalued-stock screening using multi-indicator feature engineering and backtesting. Using historical OHLCV data from S\&P 500 stocks (2010--2023), the system performs robust preprocessing, computes technical indicators, reduces redundancy via correlation-based feature selection, trains multiple ML classifiers, and selects the best model using robust criteria. Two enhancements are introduced: (1) market regime detection (Bull/Bear/Sideways) derived from past-only rolling returns and used as an additional feature; and (2) robust model selection using Balanced Accuracy to avoid biased models that trivially predict a single class. Trading signals (Buy/Hold/Sell) are generated from model confidence scores and evaluated via backtesting using total return, maximum drawdown, Sharpe ratio, and win rate. The implementation is reproducible (offline-first CSV pipeline) and produces review-ready outputs such as model comparison tables, radar visualizations, and buy-list recommendations.
\end{abstract}

\begin{IEEEkeywords}
Algorithmic Trading, Stock Prediction, Technical Indicators, Feature Selection, Market Regimes, Balanced Accuracy, Backtesting
\end{IEEEkeywords}

% =========================================================
\section{Introduction}
Stock markets exhibit high volatility and non-stationary behavior, making reliable trading decisions difficult using manual analysis alone. Machine learning methods can learn patterns from OHLCV time series and engineered indicators; however, practical trading systems must address: (i) evaluation leakage due to incorrect train/test splits on time series; (ii) overfitting from redundant indicators; and (iii) inadequate risk-aware validation where classification performance is reported without trading performance metrics. Motivated by these limitations, we develop an end-to-end ML pipeline that includes data cleaning, multi-indicator feature engineering, model assortment, robust model selection, signal generation, and backtesting.

% =========================================================
\section{Related Work}
Neural classifiers and backtesting frameworks have been proposed for trading systems and prior literature emphasizes robust evaluation. Parente \emph{et al.} demonstrated regime-based evaluation in cryptocurrencies \cite{Parente2024}. Bhuiyan \emph{et al.} highlighted common pitfalls such as overfitting and evaluation leakage \cite{Bhuiyan2025}. Risk-aware reinforcement learning approaches have been explored using Mean--VaR objectives \cite{Jin2023}. Yuferova studied the impact of algorithmic trading on market efficiency \cite{Yuferova2024}, and Park \emph{et al.} showed that macro features can improve trading performance in RL settings \cite{Park2024}. Our work is inspired by multi-indicator model assortment strategies \cite{Sukma2024}, with additional safeguards for leakage-free evaluation, regime awareness, and robust model selection.

% =========================================================
\section{Dataset Description}
\subsection{Data Source and Scope}
We use daily OHLCV data for S\&P 500 stocks over the period 2010--2023. The raw fields include Open, High, Low, Close, and Volume. The implementation supports offline-first execution using locally stored CSV files for repeatability.

\subsection{Preprocessing}
To ensure time-series correctness and reliable indicator computation, preprocessing includes:
\begin{itemize}
\item Date parsing and ascending sorting.
\item Duplicate-date removal.
\item Numeric conversion and removal of invalid entries (NaN/Inf).
\end{itemize}

% =========================================================
\section{Experimental Setup}
This section summarizes the experimental configuration used to produce the reported results.
\begin{itemize}
\item \textbf{Universe:} S\&P 500 subset (50 tickers from \texttt{SP500.csv}).
\item \textbf{Period:} 2010--2023 (daily OHLCV).
\item \textbf{Label mode (REALISTIC):} future direction label with horizon $h=5$ trading days.
\item \textbf{Holdout split (leakage-free):} Train $\le$ 2021-12-31; Test = 2022-01-01 to 2022-12-31 (Table~\ref{tab:table7}).
\item \textbf{Models:} GBM, Random Forest, SVM (linear), Logistic Regression, MLP, Decision Tree, KNN, GaussianNB.
\item \textbf{Feature set:} paper-aligned indicators (MA(20,50), RSI, MACD, BB, OBV, Ichimoku) plus the proposed \texttt{Regime} feature.
\item \textbf{Robust selection:} best model selected using Balanced Accuracy.
\item \textbf{Trading output:} threshold-based Buy/Hold/Sell and Top-$N$ confidence ranking (when probabilities concentrate near 0.5).
\end{itemize}

% =========================================================
\section{Data Statistics}
Table~\ref{tab:datastats} summarizes the dataset size and label distribution observed in our experiment run. (Regime counts can be reported similarly; see Appendix for code.)
\begin{table}[!t]
\caption{Dataset statistics (S\&P 500 subset experiment).}
\label{tab:datastats}
\centering
\begin{tabular}{lc}
\toprule
Item & Value \\
\midrule
Total observations (rows) & 132,843 \\
Label = 1 count & 73,716 \\
Label = 0 count & 59,127 \\
Positive ratio (Label=1) & 0.555 \\
Negative ratio (Label=0) & 0.445 \\
\bottomrule
\end{tabular}
\end{table}

% =========================================================
\section{System Architecture and Implementation}
The system architecture consists of three layers:
\begin{enumerate}
\item \textbf{Data Layer:} ticker loading, OHLCV acquisition, cleaning, and dataset construction.
\item \textbf{ML Layer:} feature selection, model training, evaluation, and robust best-model selection.
\item \textbf{Trading \& Reporting Layer:} signal generation, buy-list extraction, and portfolio backtesting.
\end{enumerate}
All intermediate artifacts (datasets, model bundle, signals, and plots) are stored under an outputs directory for reproducible experiments.

% =========================================================
\section{Methodology}
\subsection{Technical Indicator Feature Engineering}
From cleaned OHLCV data, we compute indicators capturing trend, momentum, volatility, and volume confirmation. Key indicators include MA(20,50), RSI, MACD, Bollinger Bands, OBV, and Ichimoku base line.

\textbf{Simple Moving Average (SMA):}
\begin{equation}
SMA_n(t)=\frac{1}{n}\sum_{i=0}^{n-1} Close_{t-i}
\end{equation}

\textbf{Exponential Moving Average (EMA):}
\begin{equation}
EMA_t=\alpha Close_t+(1-\alpha)EMA_{t-1}, \quad \alpha=\frac{2}{n+1}
\end{equation}

\textbf{MACD:}
\begin{equation}
MACD=EMA_{12}-EMA_{26}
\end{equation}

\textbf{Bollinger Upper Band:}
\begin{equation}
UpperBB=SMA_{20}+2\sigma_{20}
\end{equation}

\subsection{Label Definition}
\textbf{REALISTIC (future direction):}
\begin{equation}
label_t=
\begin{cases}
1, & Close_{t+h} > Close_t \\
0, & \text{otherwise}
\end{cases}
\end{equation}
where $h=5$ in our experiment.

\subsection{Feature Selection}
To reduce redundancy and multicollinearity, Pearson correlation is used:
\begin{equation}
C_{ij}=\frac{Cov(I_i,I_j)}{\sigma(I_i)\sigma(I_j)}
\end{equation}
If $|C_{ij}|>0.75$, one of the correlated indicators is removed.

% =========================================================
\section{Proposed Enhancements (Novelties)}
\subsection{Novelty A: Market Regime Detection}
Market behavior varies across bull, bear, and sideways regimes. We compute past-only rolling log returns:
\begin{equation}
r_t=\ln\left(\frac{Close_t}{Close_{t-1}}\right)
\end{equation}
\begin{equation}
\mu_t=\frac{1}{L}\sum_{i=0}^{L-1} r_{t-i}
\end{equation}
Regime assignment:
\begin{equation}
Regime_t=
\begin{cases}
2\ (\text{Bull}), & \mu_t>\epsilon \\
0\ (\text{Bear}), & \mu_t<-\epsilon \\
1\ (\text{Sideways}), & |\mu_t|\le\epsilon
\end{cases}
\end{equation}
where $L=20$ and $\epsilon=0.0005$ in our implementation.

\subsection{Novelty B: Robust Model Selection Using Balanced Accuracy}
To avoid biased ``always BUY'' models, selection is based on Balanced Accuracy:
\begin{equation}
Recall=\frac{TP}{TP+FN},\quad Specificity=\frac{TN}{TN+FP}
\end{equation}
\begin{equation}
BalancedAcc=\frac{Recall+Specificity}{2}
\end{equation}

% =========================================================
\section{Algorithms}
\subsection{Algorithm 1: Identify Undervalued Stocks}
\begin{algorithm}[!t]
\caption{Identify Undervalued Stocks (BUY List Generation)}
\begin{algorithmic}[1]
\Require S\&P 500 tickers, OHLCV data, optional fundamental ratios
\Ensure BUY shortlist and ML dataset
\For{each ticker}
  \State Acquire OHLCV (offline CSV / API fallback)
  \State Preprocess (sort date, remove duplicates, handle invalid values)
  \State Compute indicators and optional regime feature
  \State Compute undervaluation signal / label (if using PAPER\_US mode)
  \If{criteria satisfied}
    \State Add stock to BUY list
  \EndIf
\EndFor
\State Save dataset with \texttt{symbol}, \texttt{date}, features, and \texttt{label}
\end{algorithmic}
\end{algorithm}

\subsection{Algorithm 2: Machine Learning Model Assortment}
\begin{algorithm}[!t]
\caption{Machine Learning Model Assortment and Robust Selection}
\begin{algorithmic}[1]
\Require Dataset $(X,Y)$, models set $\{\text{GBM,SVM,RF,LR,MLP,DT,KNN,NB}\}$
\Ensure Best model bundle and performance report
\State Feature selection (correlation-based or fixed selected set)
\State Time-aware train/test split (leakage-free holdout)
\State Impute missing values using training statistics only
\If{class imbalance exists}
  \State Apply SMOTE on training set only
\EndIf
\For{each model}
  \State Train on $(X_{train},Y_{train})$
  \State Predict on $X_{test}$ and compute Accuracy, Precision, Recall, F1
  \State Compute Specificity and Balanced Accuracy
\EndFor
\State Select best model based on Balanced Accuracy
\State Save best model bundle (\texttt{joblib}) with feature list
\end{algorithmic}
\end{algorithm}

% =========================================================
\section{Signal Generation (Buy/Hold/Sell)}
Let $p=P(\text{label}=1)$ be the predicted confidence for the positive (Buy) class.
\begin{equation}
Signal=
\begin{cases}
1\ (\text{Buy}), & p \ge \tau_{buy}\\
-1\ (\text{Sell/Exit}), & p \le \tau_{sell}\\
0\ (\text{Hold}), & \text{otherwise}
\end{cases}
\end{equation}
In our implementation, $\tau_{buy}=0.6$ and $\tau_{sell}=0.4$. Since probabilities often concentrate near 0.5, we additionally report a Top-$N$ confidence ranking method for daily recommendations.

% =========================================================
\section{Backtesting Assumptions}
To avoid lookahead bias and keep evaluation reproducible:
\begin{itemize}
\item \textbf{Next-day execution:} selections at day $t-1$ are applied to returns of day $t$.
\item \textbf{Equal-weight portfolio:} the Top-$N$ selected stocks are equally weighted each day.
\item \textbf{No transaction costs:} transaction costs/slippage are not included; therefore, returns may be optimistic.
\end{itemize}

% =========================================================
\section{Backtesting Metrics}
Strategy return:
\begin{equation}
Return_t=Position_t\cdot\frac{Close_t-Close_{t-1}}{Close_{t-1}}
\end{equation}
Equity curve:
\begin{equation}
Equity_t = Equity_{t-1}(1+Return_t)
\end{equation}
Total return:
\begin{equation}
TotalReturn=\frac{Equity_T}{Equity_0}-1
\end{equation}
Maximum drawdown:
\begin{equation}
MDD=\min_t\left(\frac{Equity_t}{\max_{s\le t}Equity_s}-1\right)
\end{equation}
Sharpe ratio:
\begin{equation}
Sharpe=\sqrt{252}\cdot\frac{mean(Return)}{std(Return)}
\end{equation}

% =========================================================
\section{Experimental Results}
\subsection{Model Performance (Table 7)}
Table~\ref{tab:table7} summarizes test performance. GBM achieves the highest F1-score (0.582) with the strongest recall (0.691).

\begin{table}[!t]
\caption{TABLE 7. Machine learning test performance details.}
\label{tab:table7}
\centering
\begin{tabular}{lcccc}
\toprule
Model & Accuracy & Precision & Recall & F1 \\
\midrule
NeuralNetwork      & 0.519 & 0.512 & 0.638 & 0.568 \\
GaussianNB         & 0.519 & 0.513 & 0.606 & 0.555 \\
SVM                & 0.515 & 0.508 & 0.656 & 0.573 \\
LogisticRegression & 0.514 & 0.508 & 0.657 & 0.573 \\
GBM                & 0.507 & 0.502 & 0.691 & 0.582 \\
KNN                & 0.500 & 0.497 & 0.628 & 0.554 \\
DecisionTree       & 0.502 & 0.407 & 0.010 & 0.019 \\
RandomForest       & 0.497 & 0.465 & 0.102 & 0.167 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Table and Radar Outputs}
\begin{figure}[!t]
\centering
\includegraphics[width=0.95\linewidth]{TABLE_7.png}
\caption{Rendered Table-7 image output from the reporting module.}
\label{fig:table7png}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\linewidth]{FIGURE_4_radar.png}
\caption{Radar chart of ML test performance (Accuracy, Precision, Recall, F1).}
\label{fig:radar}
\end{figure}

\subsection{Recommended Buy List (Top-$N$)}
Table~\ref{tab:topn} lists the Top-10 recommendations for the latest date (2023-12-21) based on confidence ranking.

\begin{table}[!t]
\caption{Top-10 recommended stocks (latest date: 2023-12-21) using confidence ranking.}
\label{tab:topn}
\centering
\begin{tabular}{llc}
\toprule
Symbol & Company & $prob\_buy$ \\
\midrule
ALB  & Albemarle Corp                & 0.5629 \\
APD  & Air Products \& Chemicals Inc  & 0.5418 \\
AEE  & Ameren Corp                   & 0.5353 \\
LNT  & Alliant Energy Corp           & 0.5312 \\
ABBV & AbbVie Inc.                   & 0.5260 \\
AKAM & Akamai Technologies Inc       & 0.5216 \\
AON  & Aon plc                       & 0.5139 \\
APA  & Apache Corporation            & 0.5125 \\
AES  & AES Corp                      & 0.5112 \\
ALGN & Align Technology              & 0.5080 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Backtesting Results (Top-$N$ Portfolio)}
A Top-$N$ portfolio strategy ($N=10$) achieves Total Return of 44.5638 (4456.38\%), maximum drawdown of 39.59\%, Sharpe ratio of 1.5149, and win rate of 56.97\% (no transaction cost assumption).

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\linewidth]{portfolio_equity_curve.png}
\caption{Equity curve of the Top-$N$ portfolio backtest ($N=10$).}
\label{fig:equity}
\end{figure}

% =========================================================
\section{Ablation Study (Planned)}
To quantify the contribution of the proposed enhancements, an ablation study can be performed:
\begin{itemize}
\item \textbf{Ablation-1:} Without Regime feature vs With Regime feature.
\item \textbf{Ablation-2:} Best model selected by F1 vs selected by Balanced Accuracy.
\end{itemize}
Table~\ref{tab:ablation} provides a template for reporting.
\begin{table}[!t]
\caption{Ablation study template (fill with measured values).}
\label{tab:ablation}
\centering
\begin{tabular}{lccccc}
\toprule
Setting & Best Model & F1 & BalAcc & TotalReturn & Sharpe \\
\midrule
Baseline (no Regime) & -- & -- & -- & -- & -- \\
+Regime              & -- & -- & -- & -- & -- \\
F1-based selection   & -- & -- & -- & -- & -- \\
BalAcc-based selection & -- & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

% =========================================================
\section{Discussion}
The system demonstrates an end-to-end workflow from data ingestion to portfolio evaluation. Time-aware splits reduce evaluation leakage, and correlation-based feature selection reduces redundancy among indicators. Market regime detection adds contextual information, while Balanced Accuracy based selection avoids biased models. Portfolio backtesting provides risk-adjusted validation beyond classification metrics.

% =========================================================
\section{Limitations and Future Work}
The backtest results assume zero transaction costs and slippage; incorporating realistic costs and position sizing will improve deployment realism. Fundamental ratios require reliable data sources beyond OHLCV and can be integrated in future work. Additional improvements include probability calibration for threshold-based actions and explainability (e.g., SHAP) to interpret model decisions.

% =========================================================
\section*{Reproducibility}
The pipeline saves artifacts under \texttt{outputs/} including: \texttt{timeseries\_dataset.csv}, \texttt{best\_model\_*.joblib} (model + feature list), \texttt{trading\_signals\_best\_model.csv}, \texttt{recommended\_buy\_list.csv}, and report figures (Table-7 image, radar chart, equity curve).

% =========================================================
\section*{Acknowledgment}
We thank Dr.~D.~Sasireka for guidance and support.

% =========================================================
\appendices
\section{Scripts to Compute Additional Statistics}
The following scripts can be used to compute regime distribution and confusion matrix for the best model.

\subsection{Regime Distribution}
\begin{verbatim}
import pandas as pd
df = pd.read_csv("outputs/timeseries_dataset.csv")
print(df["Regime"].value_counts())
\end{verbatim}

\subsection{Confusion Matrix (Best Model)}
\begin{verbatim}
import pandas as pd
from sklearn.metrics import confusion_matrix

m = pd.read_csv("outputs/timeseries_dataset.csv")
# use your model inference on test period, then:
# print(confusion_matrix(y_true, y_pred))
\end{verbatim}

% =========================================================
\begin{thebibliography}{99}

\bibitem{Sukma2024}
N. Sukma and C. S. Namahoot, ``An Algorithmic Trading Approach Merging Machine Learning With Multi-Indicator Strategies for Optimal Performance,'' \emph{IEEE Access}, 2024, doi: 10.1109/ACCESS.2024.3516053.

\bibitem{Parente2024}
M. Parente, L. Rizzuti, and M. Trerotola, ``A Profitable Trading Algorithm for Cryptocurrencies Using a Neural Network Model,'' \emph{Expert Systems With Applications}, vol. 238, Art. no. 121806, 2024.

\bibitem{Bhuiyan2025}
M. S. M. Bhuiyan, M. A. Rafi, G. N. Rodrigues, M. N. H. Mir, A. Ishraq, M. F. Mridha, and J. Shin, ``Deep Learning for Algorithmic Trading: A Systematic Review of Predictive Models and Optimization Strategies,'' \emph{Array}, vol. 26, Art. no. 100390, 2025.

\bibitem{Jin2023}
B. Jin, ``A Mean--VaR Based Deep Reinforcement Learning Framework for Practical Algorithmic Trading,'' \emph{IEEE Access}, 2023.

\bibitem{Yuferova2024}
D. Yuferova, ``Algorithmic Trading and Market Efficiency Around the Introduction of the NYSE Hybrid Market,'' \emph{Journal of Financial Markets}, vol. 69, Art. no. 100909, 2024.

\bibitem{Park2024}
J.-H. Park, J.-H. Kim, and J.-H. Huh, ``Deep Reinforcement Learning Robots for Algorithmic Trading: Considering Stock Market Conditions and U.S. Interest Rates,'' \emph{IEEE Access}, 2024.

\bibitem{Li2019}
Y. Li, W. Zheng, and Z. Zheng, ``Deep Robust Reinforcement Learning for Practical Algorithmic Trading,'' \emph{IEEE Access}, 2019.

\bibitem{Wang2024}
J. Wang, H. Li, and Y. Chen, ``Enhancing Trading Decision in Financial Markets: An Algorithmic Trading Framework With Continual Mean--Variance Optimization, Window Presetting, and Controlled Early-Stopping,'' \emph{IEEE Access}, 2024.

\bibitem{Gavalas2023}
D. Gavalas and T. Katsaounis, ``Flexible Decision Support System for Algorithmic Trading: Empirical Application on Crude Oil Markets,'' \emph{Expert Systems With Applications}, vol. 222, Art. no. 119834, 2023.

\bibitem{Bormann2023}
C. Bormann, Y. Zhang, and M. M{\"u}ller, ``Intelligent Algorithmic Trading Strategy Using Reinforcement Learning and Directional Change,'' \emph{Expert Systems With Applications}, vol. 227, Art. no. 120306, 2023.

\bibitem{Chen2024}
X. Chen, Z. Liu, and Y. Zhou, ``Practical Algorithmic Trading Using State Representation Learning and Imitative Reinforcement Learning,'' \emph{IEEE Access}, 2024.

\end{thebibliography}

\end{document}









